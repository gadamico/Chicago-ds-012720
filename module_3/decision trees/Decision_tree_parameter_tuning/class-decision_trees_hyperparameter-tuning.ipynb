{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Classification Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification with hyperparameter tuning\n",
    "\n",
    "### aim:\n",
    "Show classification with different strategies for the tuning and evaluation of the classifier\n",
    "1. simple __holdout__\n",
    "2. __holdout with validation__ train and validate repeatedly changing a hyperparameter, to find the value giving the best score, then test for the final score\n",
    "4. __cross validation__ on training set, then score on test set\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Workflow\n",
    "- download the data\n",
    "- drop the useless data\n",
    "- separe the predicting attributes X from the class attribute y\n",
    "- split X and y into training and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- part 1 - single run with default parameters\n",
    "    - initialise an estimator with the chosen model generator\n",
    "    - fit the estimator with the training part of X\n",
    "    - show the tree structure\n",
    "    - part 1.1 \n",
    "        - predict the y values with the fitted estimator and the train data\n",
    "            - compare the predicted values with the true ones and compute the accuracy on the training set \n",
    "    - part 1.2\n",
    "        - predict the y values with the fitted estimator and the test data\n",
    "            - compare the predicted values with the true ones and compute the accuracy on the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- part 2 - multiple runs changing a parameter\n",
    "    - prepare the structure to hold the accuracy data for the multiple runs\n",
    "    - repeat for all the values of the parameter\n",
    "        - initialise an estimator with the current parameter value\n",
    "        - fit the estimator with the training part\n",
    "        - predict the class for the test part\n",
    "        - compute the accuracy and store the value\n",
    "    - find the parameter value for the top accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- part 3 - compute accuracy with cross validation\n",
    "    - prepare the structure to hold the accuracy data for the multiple runs\n",
    "    - repeat for all the values of the parameter\n",
    "        - initialise an estimator with the current parameter value\n",
    "        - compute the accuracy with cross validation and store the value\n",
    "    - find the parameter value for the top accuracy\n",
    "    - fit the estimator with the entire X\n",
    "    - show the resulting tree and classification report\n",
    "\n",
    "The data are already in your folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input data (1599, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "random_state = 14\n",
    "np.random.seed(random_state)\n",
    "# the random state is reset here in numpy, all the scikit-learn procedure use the numpy random state\n",
    "# obviously the experiment can be repeated exactly only with a complete run of the program\n",
    "\n",
    "\n",
    "data_url = \"winequality-red.csv\"\n",
    "target_name = 'quality'\n",
    "to_drop = []\n",
    "# parameter_values to be determined after the fitting of the full tree\n",
    "df = pd.read_csv(data_url , sep = ';')\n",
    "print(\"Shape of the input data {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Have a quick look to the data.\n",
    "- use the .shape attribute to see the size\n",
    "- use the `.head()` function to see column names and some data\n",
    "- use the `.hist()` method for an histogram of the columns\n",
    "- use the .unique method to see the class values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `hist` method of the DataFrame to show the histograms of the attributes\n",
    "\n",
    "NB: a semicolon at the end of a statement suppresses the `Out[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the unique class labels (hint: use the `unique` method of pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Split the data into the predicting values X and the class y\n",
    "Drop also the columns which are not relevant for training a classifier, if any\n",
    "\n",
    "The method \"drop\" of dataframes allows to drop either rows or columns\n",
    "- the \"axis\" parameter chooses between dropping rows (axis=0) or columns (axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another quick look to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prepare a simple model selection: holdout method\n",
    "- Split X and y in train and test\n",
    "- Show the number of samples in train and test, show the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1\n",
    "\n",
    "- Initialize an estimator with the required model generator `tree.DecisionTreeClassifier(criterion=\"entropy\")`\n",
    "- Fit the estimator on the train data and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Look at the tree structure\n",
    "- the feature names are used to show the tests in the nodes\n",
    "    - they are the column names in the X\n",
    "- the class names \n",
    "    - the attribute `estimator.classes_` contains the array of classes detected in the target; if the classes are numbers they have to be transformed in strings with `str()`\n",
    "- the dept of the visualization can be limited with the parameter `max_depth` \n",
    "    \n",
    "`plt.figure(figsize = (20,20))\n",
    " tree.plot_tree(estimator\n",
    "          , filled=True\n",
    "          , feature_names = X.columns\n",
    "          , class_names = str(estimator.classes_)\n",
    "          , rounded = True\n",
    "          , proportion = True\n",
    "          , max_depth = 1\n",
    "              );`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's see how it works on training data\n",
    "- predict the target using the fitted estimator on the training data\n",
    "- compute the accuracy on the training set using `accuracy_score(<target>,<predicted_target) * 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That's more significant: how it works on test data\n",
    "- use the fitted estimator to predict using the test features\n",
    "- compute the accuracy and store it on a variable for the final summary\n",
    "- store the maximum depth of the tree, for later use \n",
    "    - `fitted_max_depth = estimator.tree_.max_depth`\n",
    "- store the range of the parameter which will be used for tuning\n",
    "    - `parameter_values = range(1,fitted_max_depth+1)`\n",
    "- print the accuracy on the test set and the maximum depth of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Optimising the tree: limit the maximum tree depth. We will use the three way splitting: `train, validation, test`. For simplicity, since we already splitted in _train_ and _test_, we will furtherly split the _train_\n",
    "- split the training set into two parts: __train_t__ and __val__\n",
    "- max_depth - pruning the tree cutting the branches which exceed max_depth\n",
    "- the experiment is repeated varying the parameter from 1 to the depth of the unpruned tree\n",
    "- the scores for the various values are collected and plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for computing the score varying the hyperparameter\n",
    "- initialise a list to contain the scores\n",
    "- loop varying `par` in `parameter_values`\n",
    "    - initialize an estimator with a DecisionTreeClassifier, using `par` as maximum depth and `entropy` as criterion\n",
    "    - fit the estimator on the `train_t` part of the features and the target \n",
    "    - predict with the estimator using the validation features\n",
    "    - compute the score comparing the prediction with the validation target and append it to the end of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot the results\n",
    "Plot using the `parameter_values` and the list of `scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(32,20))\n",
    "#plt.plot(parameter_values, scores, '-o', linewidth=5, markersize=24)\n",
    "#plt.xlabel('max_depth')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.title(\"Score with validation varying max_depth of tree\", fontsize = 24)\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the tree after validation and print summary\n",
    "- store the parameter value giving the best score with `np.argmax(scores)`\n",
    "- initialize an estimator as a DecisionTreeClassifier, using the best parameter value computed above as maximum depth and `entropy` as criterion\n",
    "- fit the estimator using the `train` part\n",
    "- use the fitted estimator to predict using the test features\n",
    "- compute the accuracy on the test and store it on a variable for the final summary\n",
    "- print the accuracy on the test set and the best parameter value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3 - Tuning with __Cross Validation__\n",
    "Optimisation of the hyperparameter with __cross validation__ (cv suffix in the variable names).\n",
    "Now we will tune the hyperparameter looping on cross validation with the __training set__, then we will fit the estimator on the training set and evaluate the performance on the __test set__\n",
    "\n",
    "- initialize an empty list for the scores\n",
    "- loop varying `par` in `parameter_values`\n",
    "    - initialize an estimator with a DecisionTreeClassifier, using `par` as maximum depth and `entropy` as criterion\n",
    "    - compute the score using the estimator on the `train` part of the features and the target using \n",
    "        - `cross_val_score(estimator, X_train, y_train, scoring='accuracy', cv = 5)`\n",
    "        - the result is list of scores\n",
    "    - compute the average of the scores and append it to the end of the list\n",
    "- print the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot using the `parameter_values` and the list of `scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(32,20))\n",
    "#plt.plot(parameter_values,scores,'-o',linewidth=5,markersize=25)\n",
    "#plt.xlabel('max_depth')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.title(\"Score with validation varying max_depth of tree\", fontsize = 24)\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the tree after cross validation and print summary\n",
    "- store the parameter value giving the best score with `np.argmax(scores)`\n",
    "- initialize an estimator as a DecisionTreeClassifier, using the best parameter value computed above as maximum depth and `entropy` as criterion\n",
    "- fit the estimator using the `train` part\n",
    "- use the fitted estimator to predict using the test features\n",
    "- compute the accuracy on the test and store it on a variable for the final summary\n",
    "- print the accuracy on the test set and the best parameter value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **micro**:\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "- **macro**:\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "- **weighted**:\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a summary of the four experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Observations made from the above four experiments:\n",
    "1) By looking at the difference between the accuracies of train at first  and test data later\n",
    "2) accuracy of train data reaches to almost 100% which represents the no bias\n",
    "3) accuracy of test data also varies a lot of difference from the accurancy of train data which represents the high variance\n",
    "4) In machine learning language , a model which is in above two points situation means, model is overfitted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Suggested exercises\n",
    "- try other datasets\n",
    "- try to optimise the parameters \"min_impurity_decrease\" \"min_samples_leaf\" and \"min_samples_split\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
